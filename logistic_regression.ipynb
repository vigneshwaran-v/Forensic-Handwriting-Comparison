{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Requirements\n",
      "Imported Images\n",
      "Cropped Images\n",
      "150 65\n",
      "Resized Images\n",
      "Processed and Written Images\n",
      "Similar Sample generated\n",
      "Different Sample generated\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 19 20:21:08 2018\n",
    "\n",
    "@author: Vicky\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import cv2, os\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_SIZE = 100000\n",
    "\n",
    "print(\"Imported Requirements\")\n",
    "## Import images\n",
    "mypath='V:\\\\Sem_2\\\\674\\\\pa1\\\\Project_1_SML\\\\Dataset[Without-Features]\\\\AND_Images[WithoutFeatures]'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "images = np.empty(len(onlyfiles), dtype=object)\n",
    "for n in range(0, len(onlyfiles)):\n",
    "    images[n] = cv2.imread( join(mypath,onlyfiles[n]),cv2.IMREAD_GRAYSCALE )\n",
    "print(\"Imported Images\")\n",
    "\n",
    "## Crop images ##\n",
    "images = [i[~np.all(i == 255, axis=1)][:,~np.all(i[~np.all(i == 255, axis = 1)] == 255, axis=0)] for i in images]\n",
    "print(\"Cropped Images\")\n",
    "\n",
    "## Resize images ##\n",
    "shapes = [i.shape for i in images]\n",
    "height = [i[0] for i in shapes]\n",
    "length = [i[1] for i in shapes]\n",
    "avg_height = int(sum(height)/len(height))\n",
    "avg_length = int(sum(length)/len(length))\n",
    "print(avg_length, avg_height)\n",
    "images = [cv2.resize(images[i],(avg_length, avg_height)) for i in range(len(images))]\n",
    "print(\"Resized Images\")\n",
    "\n",
    "## Write Processed images ##\n",
    "processed_directory = \"Processed/\"\n",
    "subtracted_directory = \"Subtracted/\"\n",
    "if not os.path.exists(processed_directory):\n",
    "    os.makedirs(processed_directory)\n",
    "if not os.path.exists(subtracted_directory):\n",
    "    os.makedirs(subtracted_directory)\n",
    "save = [cv2.imwrite(processed_directory+onlyfiles[i],images[i]) for i in range(len(images))]\n",
    "print(\"Processed and Written Images\")\n",
    "\n",
    "## Selecting similar sample ##\n",
    "authors = sorted(set([i.split(\"_\")[0][:4] for i in onlyfiles]))\n",
    "author_dict = {}\n",
    "\n",
    "for i in authors:\n",
    "    author_dict[i] = []\n",
    "continue_index = 0\n",
    "sorted_keys = sorted(author_dict.keys())\n",
    "for i in sorted_keys:\n",
    "    for j in onlyfiles[continue_index:]:\n",
    "        if i in j:\n",
    "            author_dict[i].append(j)\n",
    "        else:\n",
    "            continue_index = onlyfiles.index(j)\n",
    "            break\n",
    "\n",
    "permutated_dict = {}\n",
    "for i in authors:\n",
    "    permutated_dict[i] = []\n",
    "for i in author_dict.keys():\n",
    "    for r in itertools.product(author_dict[i], author_dict[i]):\n",
    "        if (r[0] != r[1]) and ([r[1],r[0]] not in permutated_dict[i]):\n",
    "            permutated_dict[i].append([r[0],r[1]])\n",
    "            \n",
    "similar = [j for i in permutated_dict.keys() for j in permutated_dict[i]]\n",
    "similar_sample = random.sample(similar,int(DATA_SIZE/2) if int(DATA_SIZE/2) < len(similar) else len(similar))\n",
    "print(\"Similar Sample generated\")\n",
    "\n",
    "## Selecting different sample ##\n",
    "different_keys = [i.split(\"*_*\") for i in set([\"*_*\".join(sorted([i,j])) for i,j in itertools.product(author_dict.keys(),author_dict.keys()) if i!=j])]\n",
    "diff_keys_sample = random.sample(different_keys,int(DATA_SIZE/2) if int(DATA_SIZE/2) < len(different_keys) else len(different_keys))\n",
    "different_sample = [[random.choice(author_dict[i[0]]),random.choice(author_dict[i[1]])] for i in diff_keys_sample]\n",
    "print(\"Different Sample generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar sample done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "similar_sample_data = [(images[onlyfiles.index(i[0])],images[onlyfiles.index(i[1])]) for i in similar_sample]\n",
    "print (\"Similar sample done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255 ..., 255 255 255]\n",
      " [255 255 255 ...,  62  68 255]\n",
      " [255 255 255 ...,  25  32 255]\n",
      " ..., \n",
      " [255 255 255 ...,  25  32 255]\n",
      " [255 255 255 ...,  60  32 255]\n",
      " [255 255 255 ..., 243  32 255]]\n"
     ]
    }
   ],
   "source": [
    "print(similar_sample_data[105][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different sample data generated\n"
     ]
    }
   ],
   "source": [
    "different_sample_data = [(images[onlyfiles.index(i[0])],images[onlyfiles.index(i[1])]) for i in different_sample]\n",
    "print (\"different sample data generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Subtracted data saved\n"
     ]
    }
   ],
   "source": [
    "## Save Not_subtracted data ##\n",
    "not_subtracted_directory = \"Not_Subtracted/\"\n",
    "if not os.path.exists(not_subtracted_directory):\n",
    "    os.makedirs(not_subtracted_directory)\n",
    "save = [cv2.imwrite(not_subtracted_directory+str(i)+ \"$0___\" +similar_sample[i][0].split(\".\")[0]+\"__\"+similar_sample[i][1],similar_sample_data[i][0]) for i in range(len(similar_sample_data))]\n",
    "save = [cv2.imwrite(not_subtracted_directory+str(i)+ \"$1___\" +similar_sample[i][0].split(\".\")[0]+\"__\"+similar_sample[i][1],similar_sample_data[i][1]) for i in range(len(similar_sample_data))]\n",
    "save = [cv2.imwrite(not_subtracted_directory+str(50000+i)+ \"$0___\" +different_sample[i][0].split(\".\")[0]+\"__\"+different_sample[i][1],different_sample_data[i][0]) for i in range(len(different_sample_data))]\n",
    "save = [cv2.imwrite(not_subtracted_directory+str(50000+i)+ \"$1___\" +different_sample[i][0].split(\".\")[0]+\"__\"+different_sample[i][1],different_sample_data[i][1]) for i in range(len(different_sample_data))]\n",
    "print(\"Not Subtracted data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported sift source Images\n"
     ]
    }
   ],
   "source": [
    "## Import images for SIFT keypoints generation\n",
    "sift_source_path='V:\\\\Sem_2\\\\674\\\\pa1\\\\Project_1_SML\\\\Not_Subtracted'\n",
    "sift_source_files = [ f for f in listdir(sift_source_path) if isfile(join(sift_source_path,f)) ]\n",
    "sift_source_images = np.empty(len(sift_source_files), dtype=object)\n",
    "for n in range(0, len(sift_source_files)):\n",
    "    sift_source_images[n] = cv2.imread( join(sift_source_path,sift_source_files[n]),cv2.IMREAD_GRAYSCALE )\n",
    "print(\"Imported sift source Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sift_keypoints_directory = \"sift_keypoints/\"\n",
    "if not os.path.exists(sift_keypoints_directory):\n",
    "    os.makedirs(sift_keypoints_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(sift_source_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sift keypoint images generated\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sift_source_files)):\n",
    "    sift_img = cv2.imread(join(sift_source_path,sift_source_files[i]))\n",
    "    sift_gray= cv2.cvtColor(sift_img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    sift_keypoints = sift.detect(sift_gray,None)\n",
    "    cv2.drawKeypoints(sift_gray,sift_keypoints,sift_img)\n",
    "    cv2.imwrite(sift_keypoints_directory+'skp_'+sift_source_files[i],sift_img)\n",
    "    \n",
    "print(\"sift keypoint images generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sift keypoints and descriptors generated\n"
     ]
    }
   ],
   "source": [
    "sift_keypoints=[]\n",
    "sift_descriptors=[]\n",
    "for i in range(len(sift_source_files)):\n",
    "    sift_img = cv2.imread(join(sift_source_path,sift_source_files[i]))\n",
    "    sift_gray= cv2.cvtColor(sift_img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    sift_kp, sift_des = sift.detectAndCompute(sift_img,None)\n",
    "    sift_keypoints.append(sift_kp)\n",
    "    sift_descriptors.append(sift_des)\n",
    "\n",
    "print(\"Sift keypoints and descriptors generated\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(sift_descriptors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 (59, 128)\n"
     ]
    }
   ],
   "source": [
    "print(sift_descriptors[1].shape[0],sift_descriptors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    0.,    0., ...,    0.,    3.,   83.],\n",
       "       [   0.,    0.,    0., ...,  128.,    1.,    0.],\n",
       "       [  63.,   67.,    1., ...,    0.,    0.,    5.],\n",
       "       ..., \n",
       "       [  10.,    1.,    0., ...,    0.,    0.,    0.],\n",
       "       [   7.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0., ...,    0.,    0.,    0.]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sift_descriptors[-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([i.shape[0] for i in sift_descriptors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial=pickle.dumps(sift_descriptors[0],protocol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 128)\n"
     ]
    }
   ],
   "source": [
    "deserial=pickle.loads(serial)\n",
    "print(deserial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrptrs=sift_descriptors[:10000]\n",
    "#print(sift_descriptors[2].shape\n",
    "with open('descriptors1.pickle', 'wb') as handle:\n",
    "    pickle.dump(descrptrs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('descriptors.pickle', 'rb') as handle:\n",
    "    unserialized_data = pickle.load(handle)\n",
    "\n",
    "print(unserialized_data[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unserialized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descrptrs=sift_descriptors[160001:200000]\n",
    "#print(sift_descriptors[2].shape\n",
    "with open('descriptors7.pickle', 'wb') as handle:\n",
    "    pickle.dump(descrptrs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeyPoint 0000023092EE18D0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sift_keypoints[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,    0.,    0.,    2.,    8.,    0.,    0.,    0.,    8.,\n",
       "          0.,    0.,   56.,  152.,    0.,    0.,    2.,  152.,    0.,\n",
       "          0.,   19.,   86.,    1.,    0.,   74.,   59.,    0.,    0.,\n",
       "         30.,   23.,    0.,    0.,   23.,    0.,    0.,    0.,    6.,\n",
       "         26.,    0.,    0.,    0.,   34.,    1.,    1.,   86.,  152.,\n",
       "          3.,    0.,    2.,  152.,    4.,    1.,   25.,   59.,    1.,\n",
       "          0.,   34.,   51.,    2.,    1.,  109.,   79.,    0.,    0.,\n",
       "          8.,    0.,    0.,    0.,    2.,   16.,    0.,    0.,    0.,\n",
       "         30.,    9.,    1.,   49.,  152.,   11.,    0.,    1.,  152.,\n",
       "        116.,   36.,   41.,   48.,    3.,    0.,    3.,   41.,   20.,\n",
       "         34.,  152.,   66.,    0.,    0.,   16.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    2.,    5.,    1.,   21.,\n",
       "         13.,    0.,    3.,    4.,   17.,   67.,   35.,   19.,    4.,\n",
       "          6.,   56.,   24.,   12.,   14.,   32.,   33.,    1.,    5.,\n",
       "         13.,    3.], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sift_descriptors[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.,    0.,    0., ...,    5.,   13.,    3.],\n",
       "       [   1.,    4.,   26., ...,    0.,    0.,    2.],\n",
       "       [  21.,   11.,    0., ...,    0.,    0.,    0.],\n",
       "       ..., \n",
       "       [   0.,    0.,    0., ...,  143.,   20.,    0.],\n",
       "       [  11.,    8.,    2., ...,    0.,    0.,    2.],\n",
       "       [   0.,    0.,    0., ...,    0.,    0.,    0.]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sift_descriptors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=30, init='k-means++', n_init=1, max_iter=10, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concatenated = []\n",
    "for i in sift_descriptors[:10000]  + sift_descriptors[100000:110000]:\n",
    "    concatenated += list(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_X = np.array(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=10,\n",
       "    n_clusters=30, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(cluster_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = km.fit_predict(cluster_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_index = list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = sift_descriptors[:10000]  + sift_descriptors[100000:110000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in desc:\n",
    "    image = [0 for k in range(30)]\n",
    "    for j in range(i.shape[0]):\n",
    "        index = cluster_index.pop()\n",
    "        image[index] += 1\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "for i in range(0,20000,2):\n",
    "    #print(i,i+1)\n",
    "    X_data.append(abs(np.array(images[i])-np.array(images[i+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_data = np.array([1 for i in range(5000)] + [0 for i in range(5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the data into training and testing ###\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "train_data[\"x\"], test_data[\"x\"], train_data[\"y\"], test_data[\"y\"] = train_test_split(X_data, Y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([[ 2, 10,  2, ...,  0,  2,  5],\n",
       "        [ 1, 19,  0, ...,  1,  2,  4],\n",
       "        [ 3,  0,  2, ...,  1,  1,  3],\n",
       "        ..., \n",
       "        [ 2,  0,  1, ...,  0,  2,  0],\n",
       "        [ 1,  0,  0, ...,  1,  3,  1],\n",
       "        [ 3,  3,  1, ...,  3,  2,  3]]), 'y': array([1, 1, 0, ..., 1, 1, 1])}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 10000\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "def train_logistic_regression(train_X,train_Y,max_iter,learn_rate):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    logisticRegr = LogisticRegression(solver = 'sag', multi_class=\"ovr\", max_iter=max_iter, C=learn_rate)\n",
    "    logisticRegr.fit(train_X, train_Y)\n",
    "    return logisticRegr\n",
    "\n",
    "def accuracy_logistic_regression(trained_model, test_X, test_Y):\n",
    "    predictions = trained_model.predict(test_X)\n",
    "    score = trained_model.score(test_X, test_Y)\n",
    "    return score\n",
    "def sgd_train_logistic_regression(train_X,train_Y,max_iterations,learn_rate):\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    sgd=SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True,shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, n_iter=2)\n",
    "    sgd.fit(train_X,train_Y)\n",
    "    return sgd\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on AND Testing Data: 0.5575\n",
      "Accuracy on AND Testing Data: 0.4765\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_logistic_regression(train_data[\"x\"],train_data[\"y\"],MAX_ITER,LEARNING_RATE)\n",
    "print(\"Accuracy on AND Testing Data: \" + str(accuracy_logistic_regression(trained_model,test_data[\"x\"],test_data[\"y\"])))\n",
    "\n",
    "trained_model_x = sgd_train_logistic_regression(train_data[\"x\"],train_data[\"y\"],MAX_ITER,LEARNING_RATE)\n",
    "print(\"Accuracy on AND Testing Data: \" + str(accuracy_logistic_regression(trained_model_x,test_data[\"x\"],test_data[\"y\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type 1 & Type 2 Error\n",
    "test_data_type1 = {\"x\":[],\"y\":[]}\n",
    "test_data_type2 = {\"x\":[],\"y\":[]}\n",
    "#calculating Type 1 & Type 2Error\n",
    "for i in range(2000):\n",
    "    if(test_data[\"y\"][i]==0):\n",
    "        test_data_type1[\"x\"].append(test_data[\"x\"][i])\n",
    "        test_data_type1[\"y\"].append(test_data[\"y\"][i])\n",
    "    else:\n",
    "        test_data_type2[\"x\"].append(test_data[\"x\"][i])\n",
    "        test_data_type2[\"y\"].append(test_data[\"y\"][i])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type1 Error: 0.391783567134\n",
      "Type2 Error: 0.493013972056\n"
     ]
    }
   ],
   "source": [
    "print(\"Type1 Error: \" + str(1-accuracy_logistic_regression(trained_model,test_data_type1[\"x\"],test_data_type1[\"y\"])))\n",
    "print(\"Type2 Error: \" + str(1-accuracy_logistic_regression(trained_model,test_data_type2[\"x\"],test_data_type2[\"y\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
